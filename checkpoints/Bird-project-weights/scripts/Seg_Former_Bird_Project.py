# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18r9VpQTc7MnfKqnPrBf8np-_4GqRwQJ5

### Steps Covered in this Tutorial

* Install SegFormer dependencies
* Download custom SegFormer semantic segmentation data
* Run SegFormer training
* Evaluate SegFormer performance
* Run SegFormer inference on test images

"""

#%%
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger
from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation
from evaluate import load
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import os
from PIL import Image
import numpy as np
import random
import roboflow
from roboflow import Roboflow
import cv2
import json
import torch.nn.functional as F
from pytorch_lightning.loggers import TensorBoardLogger


from transformers import SegformerConfig, SegformerForSemanticSegmentation
#%%
print('ok')




#%%
roboflow.login()
#roboflow.login(force=True)
rf = Roboflow()

project = rf.workspace("bird-project-8tpx7").project("cameras_combined")
dataset = project.version(4).download("png-mask-semantic")


#%%
class SemanticSegmentationDataset(Dataset):
    """Image (semantic) segmentation dataset."""

    def __init__(self, root_dir, feature_extractor):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor

        self.classes_csv_file = os.path.join(self.root_dir, "_classes.csv")
        with open(self.classes_csv_file, 'r') as fid:
            data = [l.split(',') for i,l in enumerate(fid) if i !=0]
        self.id2label = {x[0]:x[1] for x in data}

        image_file_names = [f for f in os.listdir(self.root_dir) if '.jpg' in f]
        mask_file_names = [f for f in os.listdir(self.root_dir) if '.png' in f]

        self.images = sorted(image_file_names)
        self.masks = sorted(mask_file_names)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):

        image = Image.open(os.path.join(self.root_dir, self.images[idx]))
        segmentation_map = Image.open(os.path.join(self.root_dir, self.masks[idx]))

        # randomly crop + pad both image and segmentation map to same size
        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors="pt")

        for k,v in encoded_inputs.items():
          encoded_inputs[k].squeeze_() # remove batch dimension

        return encoded_inputs

#%%
class SegformerFinetuner(pl.LightningModule):
    def __init__(
        self, 
        id2label, 
        train_dataloader=None, 
        val_dataloader=None, 
        test_dataloader=None, 
        metrics_interval=100,
        device=None  # New parameter
    ):
        super(SegformerFinetuner, self).__init__()
        self.metrics_interval = metrics_interval
        self.train_dl = train_dataloader
        self.val_dl = val_dataloader
        self.test_dl = test_dataloader

        # Updated id2label mapping for your project
        self.id2label = {
            0: "background",
            1: "Branch",
            2: "Camera",
            3: "Fence",
            4: "Ground",
            5: "Nest",
            6: "Tree",
            7: "Water",
        }
        self.label2id = {v: k for k, v in self.id2label.items()}
        self.num_classes = len(self.id2label)

        # self.model = SegformerForSemanticSegmentation.from_pretrained(
        #     "nvidia/segformer-b0-finetuned-ade-512-512",
        #     return_dict=True,
        #     num_labels=self.num_classes,
        #     id2label=self.id2label,
        #     label2id=self.label2id,
        #     ignore_mismatched_sizes=True,
        # )

                # Create a configuration for training from scratch
        config = SegformerConfig(
            num_labels=self.num_classes,
            id2label=self.id2label,
            label2id=self.label2id,
            # You can add additional hyperparameters if needed.
        )
        # Initialize the model with random weights
        self.model = SegformerForSemanticSegmentation(config)
        
        # Use provided device or default to available GPU/CPU
        self.device_name = device if device is not None else ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device_name)

        self.train_mean_iou = load("mean_iou")
        self.val_mean_iou = load("mean_iou")
        self.test_mean_iou = load("mean_iou")
        self.validation_step_outputs = []
        


    def forward(self, images, masks):
        outputs = self.model(pixel_values=images, labels=masks)
        return outputs

    def training_step(self, batch, batch_nb):
        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)
        outputs = self(images, masks)
        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)
        self.train_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )

        if batch_nb % self.metrics_interval == 0:
            metrics = self.train_mean_iou.compute(
                num_labels=self.num_classes,
                ignore_index=255,
                reduce_labels=False,
            )

            # Log overall metrics
            self.log("train_loss", loss)
            self.log("train_mean_iou", metrics["mean_iou"])
            self.log("train_mean_accuracy", metrics["mean_accuracy"])

            # Log per-class IoU metrics (if available)
            per_class_iou = metrics.get("iou", None)
            if per_class_iou is not None:
                for i, class_iou in enumerate(per_class_iou):
                    self.log(f"train_iou_{self.id2label[i]}", class_iou)

            return {"loss": loss, "metrics": metrics}
        else:
            return {"loss": loss}



    def validation_step(self, batch, batch_nb):

        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)  # Move tensors to GPU

        outputs = self(images, masks)

        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)

        self.val_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )

        self.validation_step_outputs.append({'val_loss': loss})

        return({'val_loss': loss})


    def on_validation_epoch_end(self):
        metrics = self.val_mean_iou.compute(
            num_labels=self.num_classes,
            ignore_index=255,
            reduce_labels=False,
        )

        avg_val_loss = torch.stack([x["val_loss"] for x in self.validation_step_outputs]).mean()
        self.log("val_loss", avg_val_loss)
        self.log("val_mean_iou", metrics["mean_iou"])
        self.log("val_mean_accuracy", metrics["mean_accuracy"])

        # Log per-class IoU metrics (if available)
        per_class_iou = metrics.get("iou", None)
        if per_class_iou is not None:
            for i, class_iou in enumerate(per_class_iou):
                self.log(f"val_iou_{self.id2label[i]}", class_iou)

        self.validation_step_outputs.clear()
        return metrics


    def test_step(self, batch, batch_nb):

        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)  # Move tensors to GPU

        outputs = self(images, masks)

        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)

        self.test_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )

        if not hasattr(self, "test_outputs"):
            self.test_outputs = []
        self.test_outputs.append(loss)

        return loss


    def on_test_epoch_end(self):
        if not hasattr(self, "test_outputs"):
            self.test_outputs = []

        metrics = self.test_mean_iou.compute(
            num_labels=self.num_classes,
            ignore_index=255,
            reduce_labels=False,
        )

        avg_test_loss = torch.stack(self.test_outputs).mean() if self.test_outputs else torch.tensor(0.0)
        self.log("test_loss", avg_test_loss)
        self.log("test_mean_iou", metrics["mean_iou"])
        self.log("test_mean_accuracy", metrics["mean_accuracy"])

        # Log per-class IoU metrics (if available)
        per_class_iou = metrics.get("iou", None)
        if per_class_iou is not None:
            for i, class_iou in enumerate(per_class_iou):
                self.log(f"test_iou_{self.id2label[i]}", class_iou)

        self.test_outputs.clear()  # Clear after each epoch


    # def configure_optimizers(self):
    #     return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)

    def configure_optimizers(self):
        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=1e-04, eps=1e-08)


    def train_dataloader(self):
        return self.train_dl

    def val_dataloader(self):
        return self.val_dl

    def test_dataloader(self):
        return self.test_dl


#%%
feature_extractor = SegformerFeatureExtractor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
feature_extractor.do_reduce_labels = False
feature_extractor.size = 128

train_dataset = SemanticSegmentationDataset(f"{dataset.location}/train/", feature_extractor)
val_dataset = SemanticSegmentationDataset(f"{dataset.location}/valid/", feature_extractor)
test_dataset = SemanticSegmentationDataset(f"{dataset.location}/test/", feature_extractor)

batch_size = 256
num_workers = 28
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)

# # This is to actually train
# segformer_finetuner = SegformerFinetuner(
#     train_dataset.id2label,
#     train_dataloader=train_dataloader,
#     val_dataloader=val_dataloader,
#     test_dataloader=test_dataloader,
#     metrics_interval=10,
# )



# Load the trained model from checkpoint
checkpoint_path = "bird_project_combined_v3.ckpt"  # Make sure this is the correct path
# Load the checkpoint on CPU first
segformer_finetuner = SegformerFinetuner.load_from_checkpoint(
    checkpoint_path,
    map_location=torch.device("cpu"),
    id2label=train_dataset.id2label,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    test_dataloader=test_dataloader,
    metrics_interval=10,
    device="cpu"  # Force initialization on CPU
)


##%%
early_stop_callback = EarlyStopping(
    monitor="val_loss",
    min_delta=0.00,
    patience=10,  #in case validation doesn't improve much in 5 epchos might change to 10 later
    verbose=False,
    mode="min",
)

checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor="val_loss")


logger = TensorBoardLogger("lightning_logs", name="3cam_v1", version="0")

trainer = pl.Trainer(
    logger=logger,
    accelerator="gpu",
    devices=1,
    # precision=16,  # Mixed precision disabled
    callbacks=[early_stop_callback, checkpoint_callback],
    max_epochs=100,
    val_check_interval=len(train_dataloader),
)


# trainer.fit(segformer_finetuner)

#%%
# trainer.save_checkpoint("bird_project_combined_v5.ckpt")


#%%
# Commented out IPython magic to ensure Python compatibility.
#  %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

#%%
# res = trainer.test(ckpt_path="last")
res = trainer.test(segformer_finetuner)


#%%
color_map = {
    0: (0, 0, 0),       # Background - Black
    1: (255, 0, 0),     # Branch - Red
    2: (0, 255, 0),     # Camera - Green
    3: (0, 0, 255),     # Fence - Blue
    4: (255, 255, 0),   # Ground - Yellow
    5: (128, 0, 128),   # Nest - Purple
    6: (0, 255, 255),   # Tree - Cyan
    7: (255, 165, 0),   # Water - Orange
}


def prediction_to_vis(prediction):
    vis_shape = prediction.shape + (3,)
    vis = np.zeros(vis_shape)
    for i,c in color_map.items():
        vis[prediction == i] = color_map[i]
    return Image.fromarray(vis.astype(np.uint8))

for batch in test_dataloader:
    images, masks = batch['pixel_values'], batch['labels']
    outputs = segformer_finetuner.model(images, masks)

    loss, logits = outputs[0], outputs[1]

    upsampled_logits = nn.functional.interpolate(
        logits,
        size=masks.shape[-2:],
        mode="bilinear",
        align_corners=False
    )
    predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()
    masks = masks.cpu().numpy()

n_plots = 4
from matplotlib import pyplot as plt
f, axarr = plt.subplots(n_plots,2)
f.set_figheight(15)
f.set_figwidth(15)
for i in range(n_plots):
    axarr[i,0].imshow(prediction_to_vis(predicted_mask[i,:,:]))
    axarr[i,1].imshow(prediction_to_vis(masks[i,:,:]))


#%%
#Predict on a test image and overlay the mask on the original image
test_idx = 0
input_image_file = os.path.join(test_dataset.root_dir,test_dataset.images[test_idx])
input_image = Image.open(input_image_file)
test_batch = test_dataset[test_idx]
images, masks = test_batch['pixel_values'], test_batch['labels']
images = torch.unsqueeze(images, 0)
masks = torch.unsqueeze(masks, 0)
outputs = segformer_finetuner.model(images, masks)

loss, logits = outputs[0], outputs[1]

upsampled_logits = nn.functional.interpolate(
    logits,
    size=masks.shape[-2:],
    mode="bilinear",
    align_corners=False
)
predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()
mask = prediction_to_vis(np.squeeze(masks))
mask = mask.resize(input_image.size)
mask = mask.convert("RGBA")
input_image = input_image.convert("RGBA")
overlay_img = Image.blend(input_image, mask, 0.5)

#%%
overlay_img









#---------------------------------------- testing w/o exporting ----------------------------------------------------

#%%
import os
import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np

# Hard-coded path for the input image
input_image_file = "/home/danielp_airlab/Downloads/cam5_frame_11250.jpg"

# Desired final output size (width, height)
target_size = (1024, 512)

# Check if the image exists at the specified path
if not os.path.exists(input_image_file):
    raise FileNotFoundError(f"Image not found at {input_image_file}")

# ---------------------------
# 1. Load the Image and Resize to Target Resolution
# ---------------------------
input_image = Image.open(input_image_file).convert("RGB")
if input_image.size != target_size:
    input_image = input_image.resize(target_size, Image.Resampling.LANCZOS)
print("Processing image size:", input_image.size)

# ---------------------------
# 2. Preprocess the Image with the Feature Extractor
# ---------------------------
# This will resize the image for model input as configured in the feature extractor.
encoded = feature_extractor(images=input_image, return_tensors="pt")
image_tensor = encoded["pixel_values"].to("cuda" if torch.cuda.is_available() else "cpu")

# ---------------------------
# 3. Run Inference
# ---------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
segformer_finetuner.model.to(device)
segformer_finetuner.model.eval()
with torch.no_grad():
    outputs = segformer_finetuner.model(pixel_values=image_tensor)
    # Support tuple output if necessary.
    logits = outputs[0] if isinstance(outputs, tuple) else outputs.logits

# ---------------------------
# 4. Upsample the Logits to the Final Output Resolution (1024x512)
# ---------------------------
# Note: F.interpolate expects size as (height, width)
upsampled_logits = F.interpolate(
    logits,
    size=(target_size[1], target_size[0]),
    mode="bilinear",
    align_corners=False
)
predicted_mask = upsampled_logits.argmax(dim=1).squeeze(0).cpu().numpy()

# ---------------------------
# 5. Create the Overlay Using Your Color Mapping Function
# ---------------------------
def prediction_to_vis(prediction):
    # Create an empty array with an extra channel for RGB
    vis_shape = prediction.shape + (3,)
    vis = np.zeros(vis_shape, dtype=np.uint8)
    for i, c in color_map.items():
        vis[prediction == i] = color_map[i]
    return Image.fromarray(vis)

mask_vis = prediction_to_vis(predicted_mask).convert("RGBA")
input_image_rgba = input_image.convert("RGBA")
overlay_img = Image.blend(input_image_rgba, mask_vis, alpha=0.5)

# ---------------------------
# 6. Display or Save the Final Overlay (1024x512)
# ---------------------------
overlay_img.show()
# You can also save it:
# overlay_img.save("final_overlay_1024x512.png")


# %%