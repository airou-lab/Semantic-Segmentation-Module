# -*- coding: utf-8 -*-
"""Copy-train-segformer-segmentation-on-custom-data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18r9VpQTc7MnfKqnPrBf8np-_4GqRwQJ5

# How to Train Segformer on Custom Data

This notebook shows training on **your own custom masks** for the SegFormer model as implemented in [the SegFormer paper](https://arxiv.org/pdf/2105.15203.pdf).

### Accompanying Blog Post

We recommend that you follow along in this notebook while reading the blog post on [How to Train Segformer](blog.roboflow.com/how-to-train-segformer-on-a-custom-dataset/), concurrently.

### Steps Covered in this Tutorial

* Install SegFormer dependencies
* Download custom SegFormer semantic segmentation data
* Run SegFormer training
* Evaluate SegFormer performance
* Run SegFormer inference on test images

### **About**

[Roboflow](https://roboflow.com) enables teams to deploy custom computer vision models quickly and accurately. Convert data from to annotation format, assess dataset health, preprocess, augment, and more. It's free for your first 1000 source images.

**Looking for a vision model available via API without hassle? Try Roboflow Train.**

![Roboflow Wordmark](https://i.imgur.com/dcLNMhV.png)
"""

#%% cell
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger
from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation
from datasets import load_metric
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import os
from PIL import Image
import numpy as np
import random
import roboflow
from roboflow import Roboflow
import cv2
import json
import torch.nn.functional as F
#%% cell

print('ok')




#%% cell
roboflow.login()

rf = Roboflow()

project = rf.workspace("research-s8evp").project("mot-bird")
dataset = project.version(5).download("png-mask-semantic")


#%% cell
class SemanticSegmentationDataset(Dataset):
    """Image (semantic) segmentation dataset."""

    def __init__(self, root_dir, feature_extractor):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor

        self.classes_csv_file = os.path.join(self.root_dir, "_classes.csv")
        with open(self.classes_csv_file, 'r') as fid:
            data = [l.split(',') for i,l in enumerate(fid) if i !=0]
        self.id2label = {x[0]:x[1] for x in data}

        image_file_names = [f for f in os.listdir(self.root_dir) if '.jpg' in f]
        mask_file_names = [f for f in os.listdir(self.root_dir) if '.png' in f]

        self.images = sorted(image_file_names)
        self.masks = sorted(mask_file_names)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):

        image = Image.open(os.path.join(self.root_dir, self.images[idx]))
        segmentation_map = Image.open(os.path.join(self.root_dir, self.masks[idx]))

        # randomly crop + pad both image and segmentation map to same size
        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors="pt")

        for k,v in encoded_inputs.items():
          encoded_inputs[k].squeeze_() # remove batch dimension

        return encoded_inputs

#%% cell
class SegformerFinetuner(pl.LightningModule):

    def __init__(self, id2label, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=100):
        super(SegformerFinetuner, self).__init__()
        #self.id2label = id2label
        self.metrics_interval = metrics_interval
        self.train_dl = train_dataloader
        self.val_dl = val_dataloader
        self.test_dl = test_dataloader


        # ✅ Updated id2label and label2id for MOT-Bird Dataset
        self.id2label = {
            0: "background",
            1: "Branch",
            2: "Fence",
            3: "Floor",
            4: "Nest",
            5: "Tree",
        }
        #self.num_classes = len(id2label.keys())
        self.label2id = {v:k for k,v in self.id2label.items()}
        self.num_classes = len(self.id2label)  # ✅ Ensure 6 classes

        self.model = SegformerForSemanticSegmentation.from_pretrained(
            "nvidia/segformer-b0-finetuned-ade-512-512",
            return_dict=False,
            num_labels=self.num_classes,
            id2label=self.id2label,
            label2id=self.label2id,
            ignore_mismatched_sizes=True,
        )

        # ✅ Move model to GPU explicitly
        self.model = self.model.to("cuda" if torch.cuda.is_available() else "cpu")

        self.train_mean_iou = load_metric("mean_iou")
        self.val_mean_iou = load_metric("mean_iou")
        self.test_mean_iou = load_metric("mean_iou")

        self.validation_step_outputs = []

    def forward(self, images, masks):
        outputs = self.model(pixel_values=images, labels=masks)
        return outputs

    def training_step(self, batch, batch_nb):

        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)  # ✅ Move tensors to GPU

        outputs = self(images, masks)

        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)

        self.train_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )
        if batch_nb % self.metrics_interval == 0:

            metrics = self.train_mean_iou.compute(
                num_labels=self.num_classes,
                ignore_index=255,
                reduce_labels=False,
            )

            metrics = {'loss': loss, "mean_iou": metrics["mean_iou"], "mean_accuracy": metrics["mean_accuracy"]}

            for k,v in metrics.items():
                self.log(k,v)

            return(metrics)
        else:
            return({'loss': loss})

    def validation_step(self, batch, batch_nb):

        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)  # ✅ Move tensors to GPU

        outputs = self(images, masks)

        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)

        self.val_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )

        self.validation_step_outputs.append({'val_loss': loss})

        return({'val_loss': loss})

    def on_validation_epoch_end(self):
        metrics = self.val_mean_iou.compute(
              num_labels=self.num_classes,
              ignore_index=255,
              reduce_labels=False,
          )

        avg_val_loss = torch.stack([x["val_loss"] for x in self.validation_step_outputs]).mean()
        val_mean_iou = metrics["mean_iou"]
        val_mean_accuracy = metrics["mean_accuracy"]

        metrics = {"val_loss": avg_val_loss, "val_mean_iou":val_mean_iou, "val_mean_accuracy":val_mean_accuracy}
        for k,v in metrics.items():
            self.log(k,v)

        self.validation_step_outputs.clear()

        return metrics

    def test_step(self, batch, batch_nb):

        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)  # ✅ Move tensors to GPU

        outputs = self(images, masks)

        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)

        self.test_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )

        if not hasattr(self, "test_outputs"):
            self.test_outputs = []
        self.test_outputs.append(loss)

        return loss


    def on_test_epoch_end(self):
        if not hasattr(self, "test_outputs"):
            self.test_outputs = []

        metrics = self.test_mean_iou.compute(
            num_labels=self.num_classes,
            ignore_index=255,
            reduce_labels=False,
        )

        avg_test_loss = torch.stack(self.test_outputs).mean() if self.test_outputs else torch.tensor(0.0)
        test_mean_iou = metrics["mean_iou"]
        test_mean_accuracy = metrics["mean_accuracy"]

        metrics = {"test_loss": avg_test_loss, "test_mean_iou": test_mean_iou, "test_mean_accuracy": test_mean_accuracy}

        for k, v in metrics.items():
            self.log(k, v)

        self.test_outputs.clear()  # Clear after each epoch

    def configure_optimizers(self):
        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)

    def train_dataloader(self):
        return self.train_dl

    def val_dataloader(self):
        return self.val_dl

    def test_dataloader(self):
        return self.test_dl


#%% cell
feature_extractor = SegformerFeatureExtractor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
feature_extractor.do_reduce_labels = False
feature_extractor.size = 128

train_dataset = SemanticSegmentationDataset(f"{dataset.location}/train/", feature_extractor)
val_dataset = SemanticSegmentationDataset(f"{dataset.location}/valid/", feature_extractor)
test_dataset = SemanticSegmentationDataset(f"{dataset.location}/test/", feature_extractor)

batch_size = 64
num_workers = 8
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)

#This is to actually train
# segformer_finetuner = SegformerFinetuner(
#     train_dataset.id2label,
#     train_dataloader=train_dataloader,
#     val_dataloader=val_dataloader,
#     test_dataloader=test_dataloader,
#     metrics_interval=10,
# )


# Load the trained model from checkpoint
checkpoint_path = "my_trained_model.ckpt"  # Make sure this is the correct path
segformer_finetuner = SegformerFinetuner.load_from_checkpoint(
    checkpoint_path,
    id2label=train_dataset.id2label,  # ✅ Required argument
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    test_dataloader=test_dataloader,
    metrics_interval=10,  # Keep the same interval as before
)


##%% cell
early_stop_callback = EarlyStopping(
    monitor="val_loss",
    min_delta=0.00,
    patience=10,
    verbose=False,
    mode="min",
)

checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor="val_loss")

trainer = pl.Trainer(
    accelerator="gpu",
    devices=1,  # ✅ Use GPU
    precision=16,  # ✅ Use mixed precision for better GPU efficiency
    callbacks=[early_stop_callback, checkpoint_callback],
    max_epochs=100,
    val_check_interval=len(train_dataloader),
)

#trainer.fit(segformer_finetuner)

#%%
#trainer.save_checkpoint("my_trained_model.ckpt")

#%%



#%% cell
# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

#%% cell
# res = trainer.test(ckpt_path="last")
res = trainer.test(segformer_finetuner)


#%% cell
color_map = {
    0: (0, 0, 0),       # Background - Black
    1: (255, 0, 0),     # Branch - Red
    2: (0, 255, 0),     # Fence - Green
    3: (0, 0, 255),     # Floor - Blue
    4: (255, 255, 0),   # Nest - Yellow
    5: (128, 0, 128)    # Tree - Purple
}

def prediction_to_vis(prediction):
    vis_shape = prediction.shape + (3,)
    vis = np.zeros(vis_shape)
    for i,c in color_map.items():
        vis[prediction == i] = color_map[i]
    return Image.fromarray(vis.astype(np.uint8))

for batch in test_dataloader:
    images, masks = batch['pixel_values'], batch['labels']
    outputs = segformer_finetuner.model(images, masks)

    loss, logits = outputs[0], outputs[1]

    upsampled_logits = nn.functional.interpolate(
        logits,
        size=masks.shape[-2:],
        mode="bilinear",
        align_corners=False
    )
    predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()
    masks = masks.cpu().numpy()

n_plots = 4
from matplotlib import pyplot as plt
f, axarr = plt.subplots(n_plots,2)
f.set_figheight(15)
f.set_figwidth(15)
for i in range(n_plots):
    axarr[i,0].imshow(prediction_to_vis(predicted_mask[i,:,:]))
    axarr[i,1].imshow(prediction_to_vis(masks[i,:,:]))


#%% cell
#Predict on a test image and overlay the mask on the original image
test_idx = 0
input_image_file = os.path.join(test_dataset.root_dir,test_dataset.images[test_idx])
input_image = Image.open(input_image_file)
test_batch = test_dataset[test_idx]
images, masks = test_batch['pixel_values'], test_batch['labels']
images = torch.unsqueeze(images, 0)
masks = torch.unsqueeze(masks, 0)
outputs = segformer_finetuner.model(images, masks)

loss, logits = outputs[0], outputs[1]

upsampled_logits = nn.functional.interpolate(
    logits,
    size=masks.shape[-2:],
    mode="bilinear",
    align_corners=False
)
predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()
mask = prediction_to_vis(np.squeeze(masks))
mask = mask.resize(input_image.size)
mask = mask.convert("RGBA")
input_image = input_image.convert("RGBA")
overlay_img = Image.blend(input_image, mask, 0.5)

#%% cell
overlay_img












#--------------------------------for exporting everything?----------------------------------

# %%
# ---------------------------
# 1. Load and Resize Your Image
# ---------------------------
# Path to any image you want to run inference on.
input_image_file = "/home/daniel_perez/Downloads/camera angles for testing/5.jpg"

# Open the image and ensure it is in RGB mode.
input_image = Image.open(input_image_file).convert("RGB")

# Define your desired size: converting 1920x1080 to 1024x512 (width, height)
target_size = (1024, 512)
if input_image.size != target_size:
    input_image = input_image.resize(target_size, Image.Resampling.LANCZOS)

# ---------------------------
# 2. Preprocess the Image
# ---------------------------
# Use your already defined feature extractor.
# (If the extractor was set up with a different size, you can override it here if needed.)
encoded = feature_extractor(images=input_image, return_tensors="pt")
image_tensor = encoded["pixel_values"]

# Move the tensor to the same device as your model.
device = "cuda" if torch.cuda.is_available() else "cpu"
segformer_finetuner.model.to(device)  # Ensure the model is on the same device as the input
image_tensor = image_tensor.to(device)

# ---------------------------
# 3. Run the Model in Inference Mode
# ---------------------------
# Set the model to evaluation mode.
segformer_finetuner.model.eval()

with torch.no_grad():
    # Note: When doing inference, you only need to pass pixel_values.
    outputs = segformer_finetuner.model(pixel_values=image_tensor)
    # The model may return a tuple; extract logits accordingly.
    if isinstance(outputs, tuple):
        logits = outputs[0]
    else:
        logits = outputs.logits

# ---------------------------
# 4. Upsample and Obtain the Predicted Mask
# ---------------------------
# Upsample the logits to the original image dimensions.
# (PIL’s size is (width, height), but PyTorch expects (height, width).)
upsampled_logits = nn.functional.interpolate(
    logits,
    size=(input_image.height, input_image.width),
    mode="bilinear",
    align_corners=False
)

# For each pixel, choose the class with the highest logit.
predicted_mask = upsampled_logits.argmax(dim=1).squeeze(0).cpu().numpy()

# ---------------------------
# 5. Visualize the Results
# ---------------------------
# Convert the predicted mask to a color image.
mask_vis = prediction_to_vis(predicted_mask)
mask_vis = mask_vis.convert("RGBA")

# Convert the original image to RGBA (required for blending)
input_image_rgba = input_image.convert("RGBA")

# Blend the original image and the segmentation mask.
overlay_img = Image.blend(input_image_rgba, mask_vis, alpha=0.5)

# Display the overlay image (or save it using overlay_img.save("output.png"))
overlay_img.show()






















#---------------------------------------- export single image + JSON ----------------------------------------------------

# %%
# Define your input image file (the one used for overlay)
input_image_file = "/home/daniel_perez/Downloads/for_export/cam1_002640.PNG"

# Ensure the model is on the proper device and in evaluation mode
device = "cuda" if torch.cuda.is_available() else "cpu"
segformer_finetuner.model.to(device)
segformer_finetuner.model.eval()

# Load the input image
input_image = Image.open(input_image_file).convert("RGB")
width, height = input_image.size

# Preprocess the image using the feature extractor
encoded = feature_extractor(images=input_image, return_tensors="pt")
pixel_values = encoded["pixel_values"].to(device)

# Run inference to obtain logits
with torch.no_grad():
    outputs = segformer_finetuner.model(pixel_values)
    logits = outputs[0] if isinstance(outputs, tuple) else outputs.logits

# Upsample logits to match the original image size
upsampled_logits = F.interpolate(
    logits,
    size=(height, width),
    mode="bilinear",
    align_corners=False
)

# Obtain the predicted segmentation mask (each pixel's value is the class ID)
predicted_mask = upsampled_logits.argmax(dim=1).squeeze(0).cpu().numpy()

# Create a directory for single image exports
export_dir = "exports/single"
os.makedirs(export_dir, exist_ok=True)

# ---------------------------
# 1. Save the Predicted Mask as a PNG File
# ---------------------------
mask_filename = os.path.splitext(os.path.basename(input_image_file))[0] + "_mask.png"
mask_filepath = os.path.join(export_dir, mask_filename)
Image.fromarray(predicted_mask.astype(np.uint8)).save(mask_filepath)

# ---------------------------
# 2. Create and Save the Overlay Image
# ---------------------------
def prediction_to_vis(prediction):
    # Define a color map: adjust colors as needed
    color_map = {
        0: (0, 0, 0),       # Background - Black
        1: (255, 0, 0),     # Branch - Red
        2: (0, 255, 0),     # Fence - Green
        3: (0, 0, 255),     # Floor - Blue
        4: (255, 255, 0),   # Nest - Yellow
        5: (128, 0, 128)    # Tree - Purple
    }
    vis = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)
    for class_id, color in color_map.items():
        vis[prediction == class_id] = color
    return Image.fromarray(vis)

mask_vis = prediction_to_vis(predicted_mask).convert("RGBA")
input_image_rgba = input_image.convert("RGBA")
overlay_img = Image.blend(input_image_rgba, mask_vis, alpha=0.5)

overlay_filename = os.path.splitext(os.path.basename(input_image_file))[0] + "_overlay.png"
overlay_filepath = os.path.join(export_dir, overlay_filename)
overlay_img.save(overlay_filepath)

# ---------------------------
# 3. Create and Save a COCO-Style JSON Annotation for This Image
# ---------------------------
coco_output = {
    "images": [],
    "annotations": [],
    "categories": []
}

# Populate the categories using your model's id2label mapping
for cat_id, cat_name in segformer_finetuner.id2label.items():
    coco_output["categories"].append({
        "id": int(cat_id),
        "name": cat_name,
        "supercategory": "none"
    })

# Add image metadata (using a fixed image id since it's only one image)
image_id = 1
coco_output["images"].append({
    "id": image_id,
    "file_name": os.path.basename(input_image_file),
    "width": width,
    "height": height
})

annotation_id = 1
# For each unique class in the predicted mask, extract contours for segmentation
for class_id in np.unique(predicted_mask):
    if class_id == 0:  # Optionally skip the background
        continue
    # Create a binary mask for the current class
    binary_mask = (predicted_mask == class_id).astype(np.uint8) * 255
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    segmentation_list = []
    for contour in contours:
        if len(contour) < 3:
            continue  # Skip invalid polygons
        segmentation_list.append(contour.flatten().tolist())
    
    if not segmentation_list:
        continue

    # Compute bounding box and area
    x, y, w, h = cv2.boundingRect(binary_mask)
    area = int(np.sum(binary_mask > 0))
    
    coco_output["annotations"].append({
        "id": annotation_id,
        "image_id": image_id,
        "category_id": int(class_id),
        "segmentation": segmentation_list,
        "bbox": [x, y, w, h],
        "area": area,
        "iscrowd": 0
    })
    annotation_id += 1

coco_json_path = os.path.join(export_dir, "annotation_coco.json")
with open(coco_json_path, "w") as f:
    json.dump(coco_output, f, indent=4)

# ---------------------------
# Final Print Statements
# ---------------------------
print("Single image export complete!")
print("Predicted mask saved at:", mask_filepath)
print("Overlay image saved at:", overlay_filepath)
print("COCO JSON annotation saved at:", coco_json_path)

# %%
