# -*- coding: utf-8 -*-
"""Copy-train-segformer-segmentation-on-custom-data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18r9VpQTc7MnfKqnPrBf8np-_4GqRwQJ5

# How to Train Segformer on Custom Data

This notebook shows training on **your own custom masks** for the SegFormer model as implemented in [the SegFormer paper](https://arxiv.org/pdf/2105.15203.pdf).

### Accompanying Blog Post

We recommend that you follow along in this notebook while reading the blog post on [How to Train Segformer](blog.roboflow.com/how-to-train-segformer-on-a-custom-dataset/), concurrently.

### Steps Covered in this Tutorial

* Install SegFormer dependencies
* Download custom SegFormer semantic segmentation data
* Run SegFormer training
* Evaluate SegFormer performance
* Run SegFormer inference on test images

### **About**

[Roboflow](https://roboflow.com) enables teams to deploy custom computer vision models quickly and accurately. Convert data from to annotation format, assess dataset health, preprocess, augment, and more. It's free for your first 1000 source images.

**Looking for a vision model available via API without hassle? Try Roboflow Train.**

![Roboflow Wordmark](https://i.imgur.com/dcLNMhV.png)
"""

#%%
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger
from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation
from datasets import load_metric
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import os
from PIL import Image
import numpy as np
import random
import roboflow
from roboflow import Roboflow
import cv2
import json
import torch.nn.functional as F
from pytorch_lightning.loggers import TensorBoardLogger

#%%
print('ok')




#%%
roboflow.login()
#roboflow.login(force=True)
rf = Roboflow()

project = rf.workspace("bird-project-8tpx7").project("cameras_combined")
dataset = project.version(4).download("png-mask-semantic")


#%%
class SemanticSegmentationDataset(Dataset):
    """Image (semantic) segmentation dataset."""

    def __init__(self, root_dir, feature_extractor):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor

        self.classes_csv_file = os.path.join(self.root_dir, "_classes.csv")
        with open(self.classes_csv_file, 'r') as fid:
            data = [l.split(',') for i,l in enumerate(fid) if i !=0]
        self.id2label = {x[0]:x[1] for x in data}

        image_file_names = [f for f in os.listdir(self.root_dir) if '.jpg' in f]
        mask_file_names = [f for f in os.listdir(self.root_dir) if '.png' in f]

        self.images = sorted(image_file_names)
        self.masks = sorted(mask_file_names)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):

        image = Image.open(os.path.join(self.root_dir, self.images[idx]))
        segmentation_map = Image.open(os.path.join(self.root_dir, self.masks[idx]))

        # randomly crop + pad both image and segmentation map to same size
        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors="pt")

        for k,v in encoded_inputs.items():
          encoded_inputs[k].squeeze_() # remove batch dimension

        return encoded_inputs

#%%
class SegformerFinetuner(pl.LightningModule):
    def __init__(
        self, 
        id2label, 
        train_dataloader=None, 
        val_dataloader=None, 
        test_dataloader=None, 
        metrics_interval=100,
        device=None  # New parameter
    ):
        super(SegformerFinetuner, self).__init__()
        self.metrics_interval = metrics_interval
        self.train_dl = train_dataloader
        self.val_dl = val_dataloader
        self.test_dl = test_dataloader

        # Updated id2label mapping for your project
        self.id2label = {
            0: "background",
            1: "Branch",
            2: "Camera",
            3: "Fence",
            4: "Ground",
            5: "Nest",
            6: "Tree",
            7: "Water",
        }
        self.label2id = {v: k for k, v in self.id2label.items()}
        self.num_classes = len(self.id2label)

        self.model = SegformerForSemanticSegmentation.from_pretrained(
            "nvidia/segformer-b0-finetuned-ade-512-512",
            return_dict=True,
            num_labels=self.num_classes,
            id2label=self.id2label,
            label2id=self.label2id,
            ignore_mismatched_sizes=True,
        )
        
        # Use provided device or default to available GPU/CPU
        self.device_name = device if device is not None else ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device_name)

        self.train_mean_iou = load_metric("mean_iou")
        self.val_mean_iou = load_metric("mean_iou")
        self.test_mean_iou = load_metric("mean_iou")
        self.validation_step_outputs = []
        
        # ... rest of your initialization code ...


    def forward(self, images, masks):
        outputs = self.model(pixel_values=images, labels=masks)
        return outputs

    def training_step(self, batch, batch_nb):

        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)  # Move tensors to GPU

        outputs = self(images, masks)

        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)

        self.train_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )
        if batch_nb % self.metrics_interval == 0:

            metrics = self.train_mean_iou.compute(
                num_labels=self.num_classes,
                ignore_index=255,
                reduce_labels=False,
            )

            metrics = {'loss': loss, "mean_iou": metrics["mean_iou"], "mean_accuracy": metrics["mean_accuracy"]}

            for k,v in metrics.items():
                self.log(k,v)

            return(metrics)
        else:
            return({'loss': loss})

    def validation_step(self, batch, batch_nb):

        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)  # Move tensors to GPU

        outputs = self(images, masks)

        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)

        self.val_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )

        self.validation_step_outputs.append({'val_loss': loss})

        return({'val_loss': loss})

    def on_validation_epoch_end(self):
        metrics = self.val_mean_iou.compute(
              num_labels=self.num_classes,
              ignore_index=255,
              reduce_labels=False,
          )

        avg_val_loss = torch.stack([x["val_loss"] for x in self.validation_step_outputs]).mean()
        val_mean_iou = metrics["mean_iou"]
        val_mean_accuracy = metrics["mean_accuracy"]

        metrics = {"val_loss": avg_val_loss, "val_mean_iou":val_mean_iou, "val_mean_accuracy":val_mean_accuracy}
        for k,v in metrics.items():
            self.log(k,v)

        self.validation_step_outputs.clear()

        return metrics

    def test_step(self, batch, batch_nb):

        images, masks = batch['pixel_values'].to(self.device), batch['labels'].to(self.device)  # Move tensors to GPU

        outputs = self(images, masks)

        loss, logits = outputs[0], outputs[1]

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=masks.shape[-2:],
            mode="bilinear",
            align_corners=False
        )

        predicted = upsampled_logits.argmax(dim=1)

        self.test_mean_iou.add_batch(
            predictions=predicted.detach().cpu().numpy(),
            references=masks.detach().cpu().numpy()
        )

        if not hasattr(self, "test_outputs"):
            self.test_outputs = []
        self.test_outputs.append(loss)

        return loss


    def on_test_epoch_end(self):
        if not hasattr(self, "test_outputs"):
            self.test_outputs = []

        metrics = self.test_mean_iou.compute(
            num_labels=self.num_classes,
            ignore_index=255,
            reduce_labels=False,
        )

        avg_test_loss = torch.stack(self.test_outputs).mean() if self.test_outputs else torch.tensor(0.0)
        test_mean_iou = metrics["mean_iou"]
        test_mean_accuracy = metrics["mean_accuracy"]

        metrics = {"test_loss": avg_test_loss, "test_mean_iou": test_mean_iou, "test_mean_accuracy": test_mean_accuracy}

        for k, v in metrics.items():
            self.log(k, v)

        self.test_outputs.clear()  # Clear after each epoch

    def configure_optimizers(self):
        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)

    def train_dataloader(self):
        return self.train_dl

    def val_dataloader(self):
        return self.val_dl

    def test_dataloader(self):
        return self.test_dl


#%%
feature_extractor = SegformerFeatureExtractor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
feature_extractor.do_reduce_labels = False
feature_extractor.size = 128

train_dataset = SemanticSegmentationDataset(f"{dataset.location}/train/", feature_extractor)
val_dataset = SemanticSegmentationDataset(f"{dataset.location}/valid/", feature_extractor)
test_dataset = SemanticSegmentationDataset(f"{dataset.location}/test/", feature_extractor)

batch_size = 96
num_workers = 12
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)

# # This is to actually train
# segformer_finetuner = SegformerFinetuner(
#     train_dataset.id2label,
#     train_dataloader=train_dataloader,
#     val_dataloader=val_dataloader,
#     test_dataloader=test_dataloader,
#     metrics_interval=10,
# )


# Load the trained model from checkpoint
checkpoint_path = "bird_project_combined_v3.ckpt"  # Make sure this is the correct path
# Load the checkpoint on CPU first
segformer_finetuner = SegformerFinetuner.load_from_checkpoint(
    checkpoint_path,
    map_location=torch.device("cpu"),
    id2label=train_dataset.id2label,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    test_dataloader=test_dataloader,
    metrics_interval=10,
    device="cpu"  # Force initialization on CPU
)


##%%
early_stop_callback = EarlyStopping(
    monitor="val_loss",
    min_delta=0.00,
    patience=5,  #in case validation doesn't improve much in 5 epchos might change to 10 later
    verbose=False,
    mode="min",
)

checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor="val_loss")


logger = TensorBoardLogger("lightning_logs", name="Combined_v1")
trainer = pl.Trainer(
    logger=logger,
    accelerator="gpu",
    devices=1,
    # precision=16,  # Mixed precision disabled
    callbacks=[early_stop_callback, checkpoint_callback],
    max_epochs=100,
    val_check_interval=len(train_dataloader),
)


# trainer.fit(segformer_finetuner)

#%%
# trainer.save_checkpoint("bird_project_combined_v2_1.ckpt")


#%%
# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

#%%
# res = trainer.test(ckpt_path="last")
res = trainer.test(segformer_finetuner)


#%%
color_map = {
    0: (0, 0, 0),       # Background - Black 
    1: (255, 0, 0),     # Branch - Red
    2: (0, 255, 0),     # Camera - Green
    3: (0, 0, 255),     # Fence - Blue
    4: (255, 255, 0),   # Ground - Yellow
    5: (128, 0, 128),   # Nest - Purple
    6: (0, 255, 255),   # Tree - Cyan
    7: (255, 165, 0),   # Water - Orange
}


def prediction_to_vis(prediction):
    vis_shape = prediction.shape + (3,)
    vis = np.zeros(vis_shape)
    for i,c in color_map.items():
        vis[prediction == i] = color_map[i]
    return Image.fromarray(vis.astype(np.uint8))

for batch in test_dataloader:
    images, masks = batch['pixel_values'], batch['labels']
    outputs = segformer_finetuner.model(images, masks)

    loss, logits = outputs[0], outputs[1]

    upsampled_logits = nn.functional.interpolate(
        logits,
        size=masks.shape[-2:],
        mode="bilinear",
        align_corners=False
    )
    predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()
    masks = masks.cpu().numpy()

n_plots = 4
from matplotlib import pyplot as plt
f, axarr = plt.subplots(n_plots,2)
f.set_figheight(15)
f.set_figwidth(15)
for i in range(n_plots):
    axarr[i,0].imshow(prediction_to_vis(predicted_mask[i,:,:]))
    axarr[i,1].imshow(prediction_to_vis(masks[i,:,:]))


#%%
#Predict on a test image and overlay the mask on the original image
test_idx = 0
input_image_file = os.path.join(test_dataset.root_dir,test_dataset.images[test_idx])
input_image = Image.open(input_image_file)
test_batch = test_dataset[test_idx]
images, masks = test_batch['pixel_values'], test_batch['labels']
images = torch.unsqueeze(images, 0)
masks = torch.unsqueeze(masks, 0)
outputs = segformer_finetuner.model(images, masks)

loss, logits = outputs[0], outputs[1]

upsampled_logits = nn.functional.interpolate(
    logits,
    size=masks.shape[-2:],
    mode="bilinear",
    align_corners=False
)
predicted_mask = upsampled_logits.argmax(dim=1).cpu().numpy()
mask = prediction_to_vis(np.squeeze(masks))
mask = mask.resize(input_image.size)
mask = mask.convert("RGBA")
input_image = input_image.convert("RGBA")
overlay_img = Image.blend(input_image, mask, 0.5)

#%%
overlay_img











#----------------------------------------------- test on images from the folder -------------------------------------------------------

#%%
# Path to your external images folder
external_images_folder = "/home/daniel_perez/Desktop/to_test"

# Get list of image files (you can add more extensions if needed)
external_image_files = [f for f in os.listdir(external_images_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

for image_file in external_image_files:
    image_path = os.path.join(external_images_folder, image_file)
    input_image = Image.open(image_path).convert("RGB")
    
    # Prepare image using the same feature extractor
    inputs = feature_extractor(images=input_image, return_tensors="pt")
    
    # Run inference through the model
    outputs = segformer_finetuner.model(**inputs)
    logits = outputs.logits
    
    # Upsample logits to match the original image size (note: PIL size is (width, height))
    upsampled_logits = nn.functional.interpolate(
        logits,
        size=input_image.size[::-1],  # reverse to (height, width)
        mode="bilinear",
        align_corners=False
    )
    predicted_mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()
    
    # Convert the prediction to a visualization
    mask_vis = prediction_to_vis(predicted_mask)
    mask_vis = mask_vis.resize(input_image.size)
    mask_vis = mask_vis.convert("RGBA")
    
    # Blend the original image with the segmentation mask
    input_image_rgba = input_image.convert("RGBA")
    overlay = Image.blend(input_image_rgba, mask_vis, alpha=0.5)
    
    # Display the overlay
    plt.figure(figsize=(10, 10))
    plt.title(f"Segmentation Overlay: {image_file}")
    plt.imshow(overlay)
    plt.axis("off")
    plt.show()
















#------------------------------------- Export JSON ------------------------------------------------------------------------------
#%%
import os
import json
import cv2
import numpy as np
from PIL import Image
import torch.nn.functional as F
from tqdm import tqdm

def prediction_to_vis(prediction):
    color_map = {
        0: (0, 0, 0),       # Background - Black
        1: (255, 0, 0),     # Branch - Red
        2: (0, 255, 0),     # Camera - Green
        3: (0, 0, 255),     # Fence - Blue
        4: (255, 255, 0),   # Ground - Yellow
        5: (128, 0, 128),   # Nest - Purple
        6: (0, 255, 255),   # Tree - Cyan
        7: (255, 165, 0)    # Water - Orange
    }
    vis = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)
    for cls, color in color_map.items():
        vis[prediction == cls] = color
    return Image.fromarray(vis)

# Set input/output directories
external_images_folder = "/home/daniel_perez/Desktop/to_test"
export_dir = os.path.join(external_images_folder, "export_v4")
overlays_dir = os.path.join(export_dir, "overlays")
masks_dir = os.path.join(export_dir, "masks")

# Create export directories
os.makedirs(overlays_dir, exist_ok=True)
os.makedirs(masks_dir, exist_ok=True)

# Get all image files
external_image_files = [
    f for f in os.listdir(external_images_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))
]

# Define categories (starting at 1)
categories = [
    {"id": 1, "name": "Branch"},
    {"id": 2, "name": "Camera"},
    {"id": 3, "name": "Fence"},
    {"id": 4, "name": "Ground"},
    {"id": 5, "name": "Nest"},
    {"id": 6, "name": "Tree"},
    {"id": 7, "name": "Water"}
]
category_map = {i: i for i in range(1, 8)}

# Dictionary for final JSON structure
export_dict = {}

# Process each image
for image_file in tqdm(external_image_files, desc="Processing images"):
    image_path = os.path.join(external_images_folder, image_file)
    input_image = Image.open(image_path).convert("RGB")
    width, height = input_image.size

    # Prepare image for model inference
    inputs = feature_extractor(images=input_image, return_tensors="pt")
    outputs = segformer_finetuner.model(**inputs)
    logits = outputs.logits

    # Resize logits to original image size
    upsampled_logits = F.interpolate(
        logits, size=(height, width), mode="bilinear", align_corners=False
    )
    predicted_mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()

    # Convert mask to visualization
    mask_vis = prediction_to_vis(predicted_mask)
    mask_vis = mask_vis.resize((width, height)).convert("RGBA")

    # Blend mask with original image
    input_image_rgba = input_image.convert("RGBA")
    overlay = Image.blend(input_image_rgba, mask_vis, alpha=0.5)

    # Save overlay and mask
    base_name, _ = os.path.splitext(image_file)
    overlay_filename = f"overlay_{base_name}.png"
    mask_filename = f"mask_{base_name}.png"
    
    overlay_path = os.path.join(overlays_dir, overlay_filename)
    mask_path = os.path.join(masks_dir, mask_filename)

    overlay.save(overlay_path)
    mask_vis.save(mask_path)

    # Initialize image-specific dictionary
    export_dict[image_file] = {
        "annotations": [],
        "categories": categories  # Copy same categories for each image
    }

    # Extract polygons for each class
    annotation_id = 1  # Reset annotation ID per image
    for cls in np.unique(predicted_mask):
        if cls == 0:  # Skip background
            continue
        binary_mask = np.uint8(predicted_mask == cls)
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            if cv2.contourArea(contour) < 10:
                continue
            segmentation = contour.flatten().tolist()
            if len(segmentation) < 6:
                continue
            x, y, w, h = cv2.boundingRect(contour)
            area = cv2.contourArea(contour)
            export_dict[image_file]["annotations"].append({
                "id": annotation_id,
                "category_id": category_map[int(cls)],  # Ensure correct ID mapping
                "segmentation": [segmentation],
                "bbox": [x, y, w, h],
                "area": area,
                "iscrowd": 0
            })
            annotation_id += 1

# Save JSON file
json_export_path = os.path.join(export_dir, "results.json")
with open(json_export_path, "w") as json_file:
    json.dump(export_dict, json_file, indent=4)

print(f"Export completed. JSON file saved to {json_export_path}")













































# ---------------------Interpolation---------------------------------

#%%
import os
import numpy as np
import torch.nn.functional as F
from PIL import Image
from tqdm import tqdm
import IPython.display as display  # For inline display in Jupyter Notebook
from matplotlib import pyplot as plt
import cv2


# ----------------------
# INTERPOLATION FUNCTION
# ----------------------

def interpolate_mask(
    mask: np.ndarray,
    kernel_size: int = 40,               # Clear but not extreme
    threshold: int = 3,                # Low threshold for more effect
    priority_order = None,              # Default None: auto determine
    kernel_shape: str = "ellipse",      # Default ellipse: smoother connections
    operations: str = "close",          # Default close: fills gaps
    adaptive_kernel: bool = True,       # Default True: adjust by region size
    confidence_weighting: bool = True,  # Default True: smooth transitions
    boundary_aware: bool = True,       # Set to False to allow more interpolation
    preserve_original: bool = True     # NEW: Set to False to see full effect
):
    """
    Advanced semantic segmentation mask interpolation and refinement.
    
    Args:
        mask: Input segmentation mask with integer class labels
        kernel_size: Size of structuring element (higher connects regions further apart)
        threshold: Confidence threshold (lower = more aggressive filling)
        priority_order: List of classes in order of increasing priority
                        If None, uses [0,4,6,2,7,3,5,1] for bird camera trap data
        kernel_shape: Shape of the structuring element
        operations: Morphological operation
        adaptive_kernel: Whether to adjust kernel size based on region size
        confidence_weighting: Whether to use distance-based confidence
        boundary_aware: Whether to preserve original boundaries
        preserve_original: Whether to preserve original non-zero pixels
        
    Returns:
        np.ndarray: The interpolated segmentation mask
    """
    # Set default priority order for bird camera trap data if not specified
    if priority_order is None:
        # Background first (lowest priority), branch and camera at high priority
        priority_order = [0, 6, 4, 7, 2, 5, 1, 3]  

    # Make a copy of the input mask
    original_mask = mask.copy()
    result_mask = np.zeros_like(mask, dtype=mask.dtype)
    
    # Create structuring element
    kernel_shapes = {
        "ellipse": cv2.MORPH_ELLIPSE,
        "rect": cv2.MORPH_RECT,
        "cross": cv2.MORPH_CROSS
    }
    
    if kernel_shape not in kernel_shapes:
        kernel_shape = "ellipse"  # Default to ellipse if invalid shape
    
    # Define operations
    ops = {
        "close": lambda img, k: cv2.morphologyEx(img, cv2.MORPH_CLOSE, k),
        "open": lambda img, k: cv2.morphologyEx(img, cv2.MORPH_OPEN, k),
        "dilate": lambda img, k: cv2.dilate(img, k),
        "erode": lambda img, k: cv2.erode(img, k),
        "open_close": lambda img, k: cv2.morphologyEx(cv2.morphologyEx(img, cv2.MORPH_OPEN, k), cv2.MORPH_CLOSE, k),
        "close_open": lambda img, k: cv2.morphologyEx(cv2.morphologyEx(img, cv2.MORPH_CLOSE, k), cv2.MORPH_OPEN, k)
    }
    
    if operations not in ops:
        operations = "close"  # Default to close if invalid operation
    
    # Determine processing order
    unique_classes = np.unique(mask)
    if priority_order:
        # Ensure all classes in the mask are processed
        missing = sorted([cls for cls in unique_classes if cls not in priority_order])
        classes = missing + list(priority_order)
    else:
        classes = sorted(unique_classes)
    
    # Initialize confidence map for region competition
    confidence_map = np.zeros_like(mask, dtype=np.float32)
    
    # Extract boundaries if needed
    if boundary_aware:
        edges = np.zeros_like(mask, dtype=np.uint8)
        for cls in classes:
            if cls == 0:  # Skip background
                continue
            binary = (original_mask == cls).astype(np.uint8) * 255
            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(edges, contours, -1, 1, 1)
    
    # Process each class
    for cls in classes:
        # Skip background if specified at index 0 and we have other classes
        if cls == 0 and classes[0] == 0 and len(classes) > 1:
            continue
            
        # Create binary mask for current class
        binary_mask = np.uint8(original_mask == cls) * 255
        
        # For adaptive kernel sizing
        if adaptive_kernel:
            # Calculate appropriate kernel size based on region size
            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                # Get average contour area
                avg_area = np.mean([cv2.contourArea(cnt) for cnt in contours])
                # Scale factor based on log of area
                area_factor = np.clip(np.log1p(avg_area) / 10, 0.5, 2.0)
                # Adjust kernel size (minimum 3)
                k_size = max(3, int(kernel_size * area_factor))
            else:
                k_size = kernel_size
        else:
            k_size = kernel_size
            
        # Create kernel for this class
        kernel = cv2.getStructuringElement(kernel_shapes[kernel_shape], (k_size, k_size))
        
        # Apply morphological operation
        processed = ops[operations](binary_mask, kernel)
        
        # Distance-based confidence if enabled
        if confidence_weighting:
            # Calculate distance transform (distance to nearest zero pixel)
            dist_transform = cv2.distanceTransform(processed, cv2.DIST_L2, 5)
            
            # Normalize distances to 0-1 range
            max_dist = np.max(dist_transform)
            if max_dist > 0:
                normalized_dist = dist_transform / max_dist
            else:
                normalized_dist = dist_transform
                
            # Weight by threshold
            class_confidence = normalized_dist * (processed > threshold)
        else:
            # Simple binary confidence
            class_confidence = np.float32(processed > threshold)
        
        # Update result based on confidence and priority
        update_mask = (class_confidence > confidence_map) & (processed > threshold)
        
        # Respect boundaries if boundary-aware
        if boundary_aware and cls > 0:  # Skip for background
            update_mask = update_mask & (edges == 0)
        
        result_mask[update_mask] = cls
        confidence_map[update_mask] = class_confidence[update_mask]
    
    # Optionally preserve original labels for non-zero regions
    if preserve_original:
        result_mask[original_mask > 0] = original_mask[original_mask > 0]
    
    return result_mask


# Convenience function for dramatic effects
def interpolate_mask_dramatic(mask):
    """
    Apply dramatic interpolation with very visible effects.
    Useful to verify that interpolation is actually being applied.
    """
    return interpolate_mask(
        mask,
        kernel_size=18,
        threshold=3,
        operations="dilate",  # Very aggressive filling
        preserve_original=False,  # Don't preserve original pixels
        boundary_aware=False,     # Allow changes at boundaries
        adaptive_kernel=False      # Adjust kernel size by region
    )


# ----------------------
# VISUALIZATION CODE
# ----------------------

def prediction_to_vis(prediction):
    """
    Convert a segmentation mask (with integer class labels) into a color image.
    """
    color_map = {
        0: (0, 0, 0),       # Background - Black
        1: (255, 0, 0),     # Branch - Red
        2: (0, 255, 0),     # Camera - Green
        3: (0, 0, 255),     # Fence - Blue
        4: (255, 255, 0),   # Ground - Yellow
        5: (128, 0, 128),   # Nest - Purple
        6: (0, 255, 255),   # Tree - Cyan
        7: (255, 165, 0),   # Water - Orange
    }
    vis = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)
    for cls, color in color_map.items():
        vis[prediction == cls] = color
    return Image.fromarray(vis)

# ----------------------------
# MAIN VISUALIZATION SCRIPT
# ----------------------------

# Define the folder with your images
external_images_folder = "/home/daniel_perez/Desktop/to_test"
external_image_files = [
    f for f in os.listdir(external_images_folder)
    if f.lower().endswith(('.png', '.jpg', '.jpeg'))
]

# Choose which interpolation to use:
# 1 = regular interpolation with preservation (might be subtle)
# 2 = regular without preserving original (more visible)
# 3 = dramatic effects (very visible)
interpolation_mode = 3

# Process each image
for image_file in tqdm(external_image_files, desc="Processing Images"):
    image_path = os.path.join(external_images_folder, image_file)
    input_image = Image.open(image_path).convert("RGB")
    
    # Prepare the image using feature extractor
    inputs = feature_extractor(images=input_image, return_tensors="pt")
    
    # Run model inference
    outputs = segformer_finetuner.model(**inputs)
    logits = outputs.logits
    
    # Upsample logits to match the original image size
    upsampled_logits = F.interpolate(
        logits,
        size=input_image.size[::-1],  # Convert (width, height) to (height, width)
        mode="bilinear",
        align_corners=False
    )
    predicted_mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()
    
    # Apply different interpolation approaches based on selected mode
    if interpolation_mode == 1:
        # Standard interpolation with default parameters
        interpolated_mask = interpolate_mask(
            predicted_mask, 
            preserve_original=True
        )
        title = "Standard Interpolation"
    elif interpolation_mode == 2:
        # Standard interpolation without preserving original
        interpolated_mask = interpolate_mask(
            predicted_mask,
            kernel_size=9,
            threshold=10,
            preserve_original=False  # Don't preserve original to see more effects
        )
        title = "Enhanced Interpolation"
    else:  # mode 3
        # Dramatic effects to clearly show the difference
        interpolated_mask = interpolate_mask_dramatic(predicted_mask)
        title = "Morphological Interpolation"
    
    # Convert masks to colored visualizations
    raw_mask_vis = prediction_to_vis(predicted_mask)
    interp_mask_vis = prediction_to_vis(interpolated_mask)
    
    # Resize to match original image and convert to RGBA for overlay
    raw_mask_vis = raw_mask_vis.resize(input_image.size).convert("RGBA")
    interp_mask_vis = interp_mask_vis.resize(input_image.size).convert("RGBA")
    
    # Create overlays
    raw_overlay = Image.blend(input_image.convert("RGBA"), raw_mask_vis, alpha=0.5)
    interp_overlay = Image.blend(input_image.convert("RGBA"), interp_mask_vis, alpha=0.5)
    
    # Create a difference mask to highlight what changed
    diff_mask = np.zeros_like(predicted_mask)
    diff_mask[predicted_mask != interpolated_mask] = 1
    diff_vis = np.zeros((diff_mask.shape[0], diff_mask.shape[1], 3), dtype=np.uint8)
    diff_vis[diff_mask == 1] = (255, 0, 255)  # Magenta for changes
    diff_img = Image.fromarray(diff_vis).resize(input_image.size).convert("RGBA")
    
    # Display using matplotlib
    plt.figure(figsize=(20, 10))
    
    # Original image
    plt.subplot(2, 3, 1)
    plt.imshow(input_image)
    plt.title("Original Image")
    plt.axis("off")
    
    # Raw prediction mask
    plt.subplot(2, 3, 2)
    plt.imshow(raw_mask_vis)
    plt.title("Raw Prediction")
    plt.axis("off")
    
    # Interpolated mask
    plt.subplot(2, 3, 3)
    plt.imshow(interp_mask_vis)
    plt.title(title)
    plt.axis("off")
    
    # Raw overlay
    plt.subplot(2, 3, 5)
    plt.imshow(raw_overlay)
    plt.title("Raw Overlay")
    plt.axis("off")
    
    # Interpolated overlay
    plt.subplot(2, 3, 6)
    plt.imshow(interp_overlay)
    plt.title(f"{title} Overlay")
    plt.axis("off")
    
    # Difference mask (what changed)
    plt.subplot(2, 3, 4)
    plt.imshow(input_image)
    plt.imshow(diff_img, alpha=0.7)
    plt.title("Changed Pixels (Magenta)")
    plt.axis("off")
    
    plt.suptitle(f"Image: {image_file}", fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # Jupyter Notebook-friendly display of just the overlays
    print(f"\n{title} comparison for {image_file}:\n")
    
    # Create a side-by-side comparison
    comparison = Image.new('RGB', (input_image.width*2, input_image.height))
    comparison.paste(raw_overlay.convert('RGB'), (0, 0))
    comparison.paste(interp_overlay.convert('RGB'), (input_image.width, 0))
    display.display(comparison)
    
    # Print stats on how many pixels changed
    percent_changed = (diff_mask == 1).sum() / diff_mask.size * 100
    print(f"Pixels changed: {percent_changed:.2f}% of image")


















#--------------------------------------------------- Interpolation Metrics ------------------------------------------------------------------------------
#%%
import numpy as np
import torch
from datasets import load_metric
import pandas as pd
from collections import defaultdict
from tqdm import tqdm
import torch.nn.functional as F
import matplotlib.pyplot as plt

# Function to evaluate segmentation metrics for batches
def evaluate_segmentation(pred_masks, true_masks, num_classes):
    metric = load_metric("mean_iou")

    # Ensure masks are numpy arrays
    pred_masks = np.array(pred_masks)
    true_masks = np.array(true_masks)

    # Add batch
    metric.add_batch(predictions=pred_masks, references=true_masks)

    # Compute IoU metrics
    result = metric.compute(
        num_labels=num_classes,
        ignore_index=255,
        reduce_labels=False
    )

    mean_iou = result["mean_iou"]
    per_class_iou = result["per_category_iou"]
    accuracy = np.mean(pred_masks.flatten() == true_masks.flatten())

    return mean_iou, per_class_iou, accuracy

# Retrieve class names and count
num_classes = len(segformer_finetuner.id2label)
class_labels = [segformer_finetuner.id2label[i] for i in range(num_classes)]

# Metrics containers
results = defaultdict(lambda: {"mean_iou": [], "accuracy": [], "per_class_iou": []})

# Evaluate metrics BEFORE and AFTER interpolation
for batch in tqdm(test_dataloader, desc="Evaluating Model"):
    images, true_masks = batch['pixel_values'], batch['labels']
    images = images.to(segformer_finetuner.device_name)

    with torch.no_grad():
        outputs = segformer_finetuner.model(pixel_values=images)
        logits = outputs.logits
        upsampled_logits = F.interpolate(logits, size=true_masks.shape[-2:], mode="bilinear", align_corners=False)
        predicted_masks = upsampled_logits.argmax(dim=1).cpu().numpy()
        true_masks_np = true_masks.cpu().numpy()

        # Evaluate BEFORE interpolation
        mean_iou, per_class_iou, accuracy = evaluate_segmentation(predicted_masks, true_masks_np, num_classes)
        results["Before"]["mean_iou"].append(mean_iou)
        results["Before"]["accuracy"].append(accuracy)
        results["Before"]["per_class_iou"].append(per_class_iou)

        # Interpolate masks (dramatic mode)
        interpolated_masks = np.array([interpolate_mask_dramatic(mask) for mask in predicted_masks])

        # Evaluate AFTER interpolation
        mean_iou, per_class_iou, accuracy = evaluate_segmentation(interpolated_masks, true_masks_np, num_classes)
        results["After"]["mean_iou"].append(mean_iou)
        results["After"]["accuracy"].append(accuracy)
        results["After"]["per_class_iou"].append(per_class_iou)

# Aggregate overall results
summary_results = {
    "Before IoU": np.mean(results["Before"]["mean_iou"]),
    "Before Accuracy": np.mean(results["Before"]["accuracy"]),
    "After IoU": np.mean(results["After"]["mean_iou"]),
    "After Accuracy": np.mean(results["After"]["accuracy"]),
}

# Per-class IoU averages
per_class_iou_before = np.mean(results["Before"]["per_class_iou"], axis=0)
per_class_iou_after = np.mean(results["After"]["per_class_iou"], axis=0)

# Create DataFrame for per-class IoU comparison
metrics_df = pd.DataFrame({
    "Class": class_labels,
    "IoU Before": per_class_iou_before,
    "IoU After": per_class_iou_after,
})

# Display results
print("\nOverall Results (Before vs After Interpolation):")
overall_df = pd.DataFrame([summary_results])
print(overall_df)

print("\nPer-Class IoU Comparison:")
print(metrics_df)

# Visualize per-class IoU improvements
ax = metrics_df.plot.bar(
    x="Class",
    y=["IoU Before", "IoU After"],
    figsize=(12, 6),
    title="Per-Class IoU Before vs After Interpolation"
)
plt.ylabel("IoU")
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', linewidth=0.5)
plt.legend(loc="upper right")
plt.tight_layout()
plt.show()

# %%
